Got it. I’ll design a scientifically feasible integration strategy that combines LLMDet’s open-vocabulary detection with iKUN’s plug-and-play referring multi-object tracking architecture. The plan will include an in-depth architectural proposal and a PyTorch-based implementation guide for evaluation on the Refer-KITTI dataset. I’ll walk through the motivation, design choices, system architecture, and technical feasibility.

I’ll let you know once the detailed proposal and demonstration are ready, along with the Chinese translation.

# Integrating LLMDet into iKUN for Open-Set Referring Multi-Object Tracking

## Motivation and Expected Benefits

**Enhancing Open-Set Detection:** iKUN’s current pipeline relies on YOLOv8 for object detection ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=the same detection results from,85)), which limits it to a fixed set of classes. By replacing YOLOv8 with **LLMDet**, an open-vocabulary detector supervised by large language models, we enable detection of arbitrary objects described by the query. LLMDet is trained with region-level short captions and image-level detailed captions as supervision ([[2501.18954\] LLMDet: Learning Strong Open-Vocabulary Object Detectors under the Supervision of Large Language Models](https://arxiv.org/abs/2501.18954#:~:text=training objectives including a standard,available at this https URL)) ([Update README.md · fushh7/LLMDet at c5523ed](https://huggingface.co/fushh7/LLMDet/commit/c5523ed2f7d4fa841760d6babb424dc8e8f8ba85#:~:text=With this dataset%2C we finetune,modal model%2C achieving mutual benefits)), giving it a strong ability to recognize long-tail categories and attributes beyond YOLO’s label space. This upgrade allows iKUN to handle open-set and rare objects mentioned in queries (e.g. “construction vehicle” or specific colors) that YOLOv8 might miss, thus improving recall for referred targets.

**Improving Language-Visual Alignment:** LLMDet’s unique **captioning capabilities** offer richer semantic information for each detection. It can generate a short textual description for each detected region (e.g. *“white sedan car”*), as well as a detailed caption for the entire scene ([Update README.md · fushh7/LLMDet at c5523ed](https://huggingface.co/fushh7/LLMDet/commit/c5523ed2f7d4fa841760d6babb424dc8e8f8ba85#:~:text=With this dataset%2C we finetune,modal model%2C achieving mutual benefits)). Incorporating these descriptions into iKUN’s referring module will strengthen the alignment between visual data and the language query. The Knowledge Unification Module (KUM) in iKUN was designed to adapt visual features with textual guidance ([iKUN: Speak to Trackers without Retraining](https://arxiv.org/html/2312.16245v2#:~:text=work%2C we propose an i,Extensive)) ([iKUN: Speak to Trackers without Retraining](https://arxiv.org/html/2312.16245v2#:~:text=,adaptive features for each description)); with LLMDet’s captions, the model can leverage explicit textual cues about object attributes (color, type, etc.) during this adaptation. We expect this to **boost referring accuracy**, as the system can confirm matches not just via learned embeddings but also via direct language similarity (e.g. matching “white car” in the query with “white sedan” from LLMDet’s region caption).

**Plug-and-Play Modularity Retained:** A key feature of iKUN is its plug-and-play design – it **freezes the tracker** and only trains the referring module, allowing easy swapping of the detection/tracking backend ([iKUN: Speak to Trackers without Retraining](https://arxiv.org/html/2312.16245v2#:~:text=Referring multi,noise based on the current)) ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=AssA%2C respectively,trained once for multiple trackers)). Integrating LLMDet aligns with this philosophy. We can replace the YOLOv8 + tracker component with LLMDet + an MOT tracker **without modifying iKUN’s core**. In fact, the iKUN paper demonstrated that using a stronger detector (Deformable DETR instead of YOLOv8) significantly improved performance (HOTA from ~44.5% to 48.8% on Refer-KITTI) while requiring no retraining of iKUN ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=AssA%2C respectively,trained once for multiple trackers)). Therefore, using LLMDet (which outperforms baseline open-vocabulary detectors ([[2501.18954\] LLMDet: Learning Strong Open-Vocabulary Object Detectors under the Supervision of Large Language Models](https://arxiv.org/abs/2501.18954#:~:text=training objectives including a standard,available at this https URL))) is expected to similarly boost tracking and referring metrics. We maintain modularity by keeping the same interface: the tracker provides object trajectories and (optionally) captions to iKUN, which then produces final referred tracks. This way, the combined system stays **flexible** – one can still plug in different detectors if needed, as LLMDet is a drop-in improvement.

**Handling Long-Tail Descriptions:** iKUN introduced a similarity calibration to tackle long-tail textual descriptions ([iKUN: Speak to Trackers without Retraining](https://arxiv.org/html/2312.16245v2#:~:text=Concretely%2C a knowledge unification module,Extensive)) by adjusting confidence based on description “frequency.” With LLMDet, we can alleviate this issue more directly. The **image-level caption** generated by LLMDet provides a comprehensive description of the scene in common language, and region-level captions turn each candidate’s visual features into words. These serve as **bridges for uncommon phrasing**: if a query uses an unusual term, LLMDet’s caption likely uses a more common synonym, helping iKUN interpret the query. For example, a query “crimson car” could be hard to match visually if “crimson” is rare, but LLMDet might caption the region as “red car,” aligning with common vocabulary. Thus, LLMDet’s language supervision helps the system handle open-set, long-tail descriptions gracefully by grounding them in familiar terms. Overall, integrating LLMDet is motivated by the expectation of **higher recall and precision for referred tracks**, especially for uncommon query targets, while preserving iKUN’s efficient two-stage tracking-to-referring pipeline ([iKUN: Speak to Trackers without Retraining](https://arxiv.org/html/2312.16245v2#:~:text=Figure 2%3A  The motivation,adaptive features for each description)) ([iKUN: Speak to Trackers without Retraining](https://arxiv.org/html/2312.16245v2#:~:text=decouple these two subtasks%3F” In,focus on the referring subtask)).

## Architecture Design Overview

**Baseline iKUN Pipeline:** First, recall iKUN’s architecture: it decouples tracking and referring. An off-the-shelf multi-object tracker processes the video frames and outputs **trajectories** of all candidate objects, independent of any language query ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=with L words,and the final outputs T)) ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=Fref in Sec,In the textual stream%2C descrip19137)). These trajectories (sequences of bounding boxes with IDs) are then passed to iKUN’s referring module. The referring module uses a two-stream network (visual and textual) with the Knowledge Unification Module (KUM) to score each trajectory for how well it matches the input description ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=T %3D ,and the final outputs T)) ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=and textual encoders from CLIP,For similarity calibration%2C pretrained)). Top-scoring trajectories matching the query are returned as the referred objects, while others are filtered out by a confidence threshold or calibration step ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=to score candidates T by,Then the similarity calibration method)). In the original design, the tracker was based on YOLOv8 detections, and iKUN utilized CLIP-based encoders for visual (ResNet-50) and text (Transformer) features ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=and textual encoders from CLIP,For similarity calibration%2C pretrained)), with the KUM adaptively merging features under textual guidance.

([GitHub - dyhBUPT/iKUN: [CVPR 2024\] iKUN: Speak to Trackers without Retraining](https://github.com/dyhBUPT/iKUN)) *Figure 1: Original iKUN framework ([iKUN: Speak to Trackers without Retraining](https://arxiv.org/html/2312.16245v2#:~:text=work%2C we propose an i,Extensive)) ([iKUN: Speak to Trackers without Retraining](https://arxiv.org/html/2312.16245v2#:~:text=Figure 2%3A  The motivation,adaptive features for each description)). The tracker (e.g. YOLOv8+SORT) runs on the video frames (left) to produce multiple object tracks (middle, all candidates). iKUN (inserted after the tracker) takes the frozen tracker’s output and the language query, then outputs only the tracks matching the query (right, highlighted box). Our goal is to replace the detection backbone with LLMDet and enrich iKUN with LLMDet’s captioning insights, without retraining the entire tracker+referring framework.*

**Replacing YOLOv8 with LLMDet (Detection & Tracking):** We integrate LLMDet as a **drop-in replacement** for YOLOv8 in the first stage of the pipeline. Instead of using YOLOv8 to detect objects per frame, we use LLMDet’s object detector on each video frame to produce bounding boxes and confidence scores for all objects. LLMDet’s detection head is open-vocabulary: during training it learned to predict classes via generated captions, so at inference it can effectively detect any salient objects and even assign them a descriptive label or embedding ([Update README.md · fushh7/LLMDet at c5523ed](https://huggingface.co/fushh7/LLMDet/commit/c5523ed2f7d4fa841760d6babb424dc8e8f8ba85#:~:text=With this dataset%2C we finetune,modal model%2C achieving mutual benefits)). For multi-object tracking, we connect LLMDet’s detections to a tracking algorithm (such as DeepSORT, ByteTrack, or iKUN’s NeuralSORT) to form trajectories across frames. Concretely, each frame’s LLMDet detections (with their bounding box, confidence, and *caption embedding*) are fed into the tracker’s association step. We use the same MOT approach as iKUN’s original (which was DeepSORT + a neural Kalman filter in NeuralSORT ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=For neural Kalman filter ,tracker is termed as NeuralSORT))). The tracker matches detections frame-to-frame based on motion and appearance. One advantage here is that we can utilize LLMDet’s **visual embeddings or captions as appearance features** for tracking: for example, the tracker can incorporate the similarity of LLMDet’s region embeddings or the textual labels when deciding if a detection in the new frame is the same object as a track from previous frame. This could improve re-identification, because an object described as “red car” in one frame should not be matched with one labeled “blue car” in another. However, to keep the system simple and modular, we can initially treat LLMDet like any detector: use IoU and motion for linking (as in ByteTrack/DeepSORT) and optionally use LLMDet’s own ROI feature (from its backbone) as the appearance descriptor in place of YOLOv8’s feature. The outcome of this stage is a set of **trajectory proposals** (candidate tracks with IDs) that cover a much broader set of object categories and attributes than before.

**Incorporating LLMDet’s Captioning into KUM:** The core novelty of integration lies in augmenting the KUM and referring inference with LLMDet’s **region-level and image-level captions**. In the original KUM, the visual stream took both global frame features and local track features, and fused them with textual guidance to produce a unified representation, which is then compared to the query embedding ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=iKUN is designed as a,In the textual stream%2C descrip19137)) ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=and textual encoders from CLIP,For similarity calibration%2C pretrained)). We modify this as follows (see next paragraph):

- **Region-Level Descriptions as Auxiliary Text:** For each candidate trajectory, LLMDet provides a short caption for the object (e.g. “white car” or “man in red shirt”) generated by its captioning head. We feed this caption (or its text embedding) into the referring module as additional context. One design is to extend KUM to a **tri-stream** attention: visual features of the track, the query text features, *and* the region caption features. The region caption essentially is a machine-generated description of the visual track, so if it aligns well with the query description, it should help boost that track’s score. We can implement a **cascade attention** mechanism (which iKUN found effective ([iKUN: Speak to Trackers without Retraining](https://arxiv.org/html/2312.16245v2#:~:text=Cascade attention,are first aggregated via cross)) ([iKUN: Speak to Trackers without Retraining](https://arxiv.org/html/2312.16245v2#:~:text=match at L538 Finally%2C we,the default design of KUM))) where the track’s visual feature is first conditioned by the query text (as originally), and then further modulated or verified by the region caption text. Concretely, we could attend the visual feature with the query text to get a query-specific visual representation, then use the region caption embedding to refine or gate that representation. If the region caption and query describe the same attributes, the attention will amplify relevant features (e.g. emphasize “red color” in the visual feature if both caption and query mention “red”). If they diverge, the model might down-weight that candidate. This effectively injects LLMDet’s linguistic knowledge into the visual feature extraction, fulfilling KUM’s purpose of “knowledge unification.” Alternatively, we can concatenate the region caption with the original query as a combined text input to the textual encoder (since the caption describes the object in the image, adding it to the query might help the model focus on matching those terms). In our PyTorch implementation, we will experiment with both approaches: (a) **Dual-text attention**: treat query and region-caption as separate text tokens that both attend to the visual features in a modified cross-attention layer; (b) **Augmented query**: simply append the region caption string to the query description (e.g. “Find the moving car on the left. Object: white car on left side.”) and encode this with CLIP’s text encoder to get a single enhanced query embedding. The hypothesis is that if the candidate’s own caption is added to the query and encoded, it will yield a high similarity only if the caption matches the query description well (implicitly performing a text-to-text check within the embedding).
- **Global Image Caption for Context:** LLMDet also produces an image-level caption describing the whole scene (e.g. “A street with several cars and a cyclist, trees on the sides…”). We use this to provide global context in KUM. Specifically, the **global frame feature** that iKUN originally used (from CLIP visual encoder on the full frame ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=iKUN is designed as a,In the textual stream%2C descrip19137))) can be augmented or replaced with an **embedding of the image caption** from a language model. The intuition is that the image caption encodes what objects and attributes are generally present. During KUM fusion, instead of purely visual global features, a text-based global context can guide feature extraction. For example, if the query is “the cyclist” and the image caption mentions a cyclist, it reinforces that such an object exists and provides contextual attributes (“a cyclist wearing blue”). Implementation-wise, we can encode the image caption with the same textual encoder as the query, and include that embedding in the text guidance used by KUM. A simple way is to average the query embedding and image-caption embedding as the textual guidance vector, under the assumption that the caption gives additional prior about relevant content. A more advanced strategy is to perform another attention: have the query attend to the image caption (or vice versa) using a small transformer, effectively merging them into a refined query representation that is aware of the scene. This would be integrated before the main KUM. For practicality, an average or concatenation with an MLP fusion can be used to combine query and image-caption features, creating a **context-aware query vector** which then guides KUM. By doing so, if the query uses a word not seen in training, the image caption might contain an alternative phrasing; their fusion will capture the intended semantics better. This step ensures the referring module is robust to wording and benefits from scene context (for instance, knowing from the caption that only two cars are present, one red and one blue, can help if the query says “the blue car”).

**Referring Inference with Multi-Modal Cues:** After integrating the above, the referring module will output a similarity score for each track, now influenced by visual features, the query text, and LLMDet’s captions. We retain the **cosine similarity scoring** and calibration as in iKUN ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=realized by temporal average pooling%2C,Net are)), but extend the confidence calculation. Specifically, the final score for a track can be a weighted combination: `score = w_v * S_visual+text + w_t * S_caption_text`, where S_visual+text is the original iKUN similarity (visual vs query embedding after KUM) and S_caption_text is a new similarity measure between the track’s region caption and the query text. The latter is computed by encoding the region caption with the textual encoder and taking cosine similarity with the query embedding (or even by simple textual similarity metrics). By adding this term, we explicitly reward tracks whose LLMDet-caption is linguistically similar to the query. This is especially useful for subtle distinctions – e.g., two people in a scene might both be detected; if the query says “person in red shirt” and LLMDet captioned one as “man in red shirt” and the other as “man in blue jacket,” the second term will heavily favor the former track. We set the weights w_v and w_t based on validation (they could be equal or tuned so that the caption-text similarity serves as a confirmatory cue). This multi-modal scoring still results in a single confidence per track, which we then pass through iKUN’s test-time calibration. Notably, the existing **similarity calibration** using pseudo-frequency ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=i wij · p tr,Here)) ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=similarity score sj from iKUN,Neural Kalman Filter)) can be updated: instead of estimating frequency of query terms from training data alone, we can also consider the frequency of terms found in LLMDet’s captions (which might align with common words). However, an even simpler adjustment is that if the region caption matches the query well, we may not need heavy calibration for rarity – the caption acts as a translator from rare phrasing to common phrasing. Thus, the integrated system handles long-tail descriptions by essentially “translating” visual content to text (via captions) and comparing to the query in the language domain in addition to the visual domain.

In summary, the architecture after integration has three main components: **LLMDet-based Tracker** (frames -> detections -> tracked trajectories with captions), **Enhanced KUM** (combining visual features, query text, and LLMDet captions), and the existing inference logic (cosine similarity and calibration to select the referred tracks). This design uses LLMDet’s outputs at two levels: (1) *Detection-level*, to increase and diversify the candidate pool, and (2) *Feature-level*, to inject semantic knowledge into the referring stage.

## PyTorch Integration Strategy

We will implement the above architecture within a PyTorch framework, capitalizing on available open-source implementations of both iKUN and LLMDet:

- **Integrating LLMDet Model:** We obtain the official LLMDet code and pretrained weights ([Update README.md · fushh7/LLMDet at c5523ed](https://huggingface.co/fushh7/LLMDet/commit/c5523ed2f7d4fa841760d6babb424dc8e8f8ba85#:~:text=%2B This is the official,Laboratory%2FLLMDet)). The model likely provides an API to input an image and output detection results (bounding boxes, class confidences, and possibly generated captions for each box and the image). We will create a detection module in PyTorch that wraps LLMDet’s forward pass. This module will take a video frame (as tensor) and output a list of detected objects, where each object entry contains: `bbox (x,y,w,h)`, `score`, `region_caption_text` (string or token IDs), and `region_feature` (the visual feature vector for that region from LLMDet’s backbone or projection layer). If LLMDet doesn’t directly give a region feature, we can extract it from an intermediate layer (e.g. the ROI pooling or transformer output for that box). We will also get the `image_caption_text` for the frame from LLMDet’s image caption head. All these outputs are easily accessible if the model has multi-task heads; if not, we may run two forward passes (one in detection mode, one in captioning mode), or design the model to output both in one pass (since training was multi-task, likely one pass yields both predictions ([Update README.md · fushh7/LLMDet at c5523ed](https://huggingface.co/fushh7/LLMDet/commit/c5523ed2f7d4fa841760d6babb424dc8e8f8ba85#:~:text=With this dataset%2C we finetune,modal model%2C achieving mutual benefits))). This PyTorch integration ensures that during inference on a video, for each frame we call LLMDet’s model to get detections and captions.
- **Tracker Implementation:** We integrate a tracker like DeepSORT or ByteTrack in PyTorch. iKUN’s codebase already provides NeuralSORT (DeepSORT + NKF) implementation ([GitHub - dyhBUPT/iKUN: [CVPR 2024\] iKUN: Speak to Trackers without Retraining](https://github.com/dyhBUPT/iKUN#:~:text=├── CLIP ├── RN50,KITTI ├── gt_template)) ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=For neural Kalman filter ,tracker is termed as NeuralSORT)). We adapt it to accept LLMDet detections. In practice, YOLOv8 detections were probably fed as text files of `bbox + confidence + class` per frame in iKUN’s original setup. We will modify that to feed `bbox + confidence` (treat all classes uniformly because open-vocab has no fixed class index) and perhaps enrich the appearance model. For appearance, DeepSORT uses a CNN (often a ReID network or the detector’s features) to get a feature vector for each detection for matching. Here we have two great options: (a) use LLMDet’s **region_feature** directly as the appearance descriptor (since it’s presumably high-dimensional and encodes the object’s visual semantics), or (b) use the **region_caption_text** by encoding it with a text encoder to a vector, as a proxy “appearance” embedding in the language space. Method (a) keeps it in vision space similar to typical trackers (likely more reliable for spatial details), while method (b) is unconventional but could match objects by described attributes (“red car” vs “blue car”). We could even combine them (concatenate visual feature and text embedding of caption as a joint descriptor for tracking). All of this will be implemented in the tracker’s matching function. The rest of the tracker remains the same (Kalman filter prediction, IoU matching for motion, then appearance matching). We ensure this module is **stand-alone** so that one could swap back another detector if needed – e.g., by implementing a detection interface class that LLMDet and YOLOv8 both conform to.
- **Extending KUM in Code:** iKUN’s model (likely defined in `model.py` ([GitHub - dyhBUPT/iKUN: [CVPR 2024\] iKUN: Speak to Trackers without Retraining](https://github.com/dyhBUPT/iKUN#:~:text=model))) will be extended. In the original PyTorch implementation, after obtaining tracklet images or features, they feed them into a visual encoder (CLIP ResNet) and textual encoder (CLIP text) and then a fusion (KUM). We will modify this forward pass as follows:
  1. **Data Preparation:** For each tracklet, besides the sequence of image crops (or the precomputed features) and the query text, we now also have the region caption text (from LLMDet) and the frame’s image caption text. We will pass the query text and image caption text through the (frozen) CLIP textual encoder to get embeddings `f_query` and `f_imgcap` (each 1024-dim, as CLIP RN50 text encoder output ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=and textual encoders from CLIP,For similarity calibration%2C pretrained))). For the region caption, since each track has one main description (we can take the caption from the first appearance of the track or an average of captions across frames if desired), we also encode it with CLIP’s text encoder to get `f_regcap` (1024-dim). These encodings are efficient since CLIP’s text model is transformer-based and small (we keep it frozen as in original iKUN ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=truth tracklets with focal loss,For similarity calibration%2C pretrained))).
  2. **Visual Feature Extraction:** Instead of relying solely on CLIP ResNet to encode visual crops, we have the option to use LLMDet’s region features. However, to avoid architectural divergence, we might stick to using CLIP visual encoder for now (since iKUN was tuned for CLIP features). But we can enhance it by concatenating global and local image information. For each tracklet, we take the *cropped object images* (e.g. track’s boxes over T frames) and feed them through CLIP’s visual encoder to get a sequence of local features. We also feed the *full-frame images* for those T frames through CLIP (or a smaller CNN) to get global context features (as done in original design ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=iKUN is designed as a,In the textual stream%2C descrip19137))). Then we have `f_local` and `f_global`. Now, to integrate LLMDet’s influence, we have two approaches in code:
     - **Approach 1 (Feature Fusion with Text):** Use the existing KUM module structure. For example, if using “cascade attention” (the chosen design ([iKUN: Speak to Trackers without Retraining](https://arxiv.org/html/2312.16245v2#:~:text=match at L538 Finally%2C we,the default design of KUM))), KUM likely performs cross-attention between `f_global` and `f_local` with text guidance. We will adapt this to also take `f_regcap` and/or `f_imgcap` as additional inputs. We might implement a multi-head attention where keys and values are visual features (`f_global`, `f_local`) and the query is the textual embedding. To incorporate two textual embeddings (query and caption), we can either average them or perform two-stage attention (cascade): first use `f_imgcap` as query on visual features to emphasize regions mentioned in the caption, then use `f_query` as query on that refined visual feature to extract the part relevant to the query. This two-stage approach fits well in code by simply sequentially applying two attention layers. The first layer’s parameters could be considered part of an **image-contextualizer** and the second layer the actual referring attention. Both layers would be learned as part of KUM training.
     - **Approach 2 (Late Score Fusion):** Simpler to implement initially, we might not change KUM’s internal at all, and instead perform the fusion at the **score level** as mentioned. That is, run iKUN’s original forward (visual+query) to get a similarity score `s_j` for track j, then separately compute a text similarity score between query and region caption `t_j`. Then combine them: `s'_j = s_j + α * t_j` (where α could correspond to f(·) from calibration, or a tunable hyperparameter). This approach would require minimal changes to the model architecture – mostly in the inference part of code (e.g. in `test.py` when computing scores, incorporate the extra similarity). Both approaches will be implemented for experimentation. Approach 2 ensures we don’t break the plug-in nature (if no caption available, t_j = 0 and it falls back to original).
- **Training and Fine-tuning:** We plan to keep most pretrained components frozen to leverage their learned knowledge (as iKUN did with CLIP encoders ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=truth tracklets with focal loss,For similarity calibration%2C pretrained))). iKUN’s referring module (KUM and scoring head) was originally trained on **ground-truth tracklets** from Refer-KITTI with a binary classification loss (focal loss) indicating whether a track matches the query ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=realized by temporal average pooling%2C,Net are)). We will do the same training procedure for the new integrated model. The difference is we now have additional inputs (captions) – but we can still train using the ground truth data. For each ground truth track that is the correct referent, we generate a pseudo region caption by using the LLMDet model on that frame (or since it’s ground truth, we might use the ground truth class/attributes to form a caption if available). The negative tracks can also have captions generated. Then we train KUM (and any new fusion weights) to output high scores for the correct track. If Approach 2 (late fusion) is used, training can still adjust the weight α or any function f for combining scores – possibly we treat it as a hyperparameter chosen to maximize validation HOTA. If Approach 1 (full feature fusion) is used, we’ll backpropagate through the new attention layers to learn how to use caption embeddings effectively. PyTorch makes it straightforward to implement these attention operations and integrate them into the autograd graph. We will verify that during training, the tracker is not being updated (we train only the referring module; the detection/tracking is decoupled and their outputs are treated as fixed inputs as per iKUN’s paradigm ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=T %3D ,and the final outputs T))).
- **Experimental Pipeline in Code:** For experiments on Refer-KITTI, we follow iKUN’s two-phase approach:
  1. **Offline Tracking with LLMDet:** We run the LLMDet detector on the Refer-KITTI video frames and use the tracker to generate candidate trajectories for each sequence. This can be done in a preprocessing script. The output is saved in the same format as the original YOLOv8-based tracks (e.g., text files with track IDs and boxes). Additionally, we save the region caption for each detection (perhaps in the file or a separate JSON keyed by track ID and frame). If using NeuralSORT with NKF, we incorporate that as well – iKUN’s code for NKF can be reused, since it only adjusts how the tracker updates motion parameters ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=3,as%3A Kk %3D P 0)) ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=and Qk,time step k − 1)) (independent of detector type). We ensure the tracker and NKF are properly configured for the new detections (likely only tuning detection confidence thresholds since LLMDet’s score distribution may differ from YOLO’s).
  2. **Referring Module Inference:** We then run iKUN’s referring module on each video. This involves loading the candidate tracks (now from LLMDet) and, for each query description, computing similarity scores. In code, this is typically `test.py` iterating over sequences and queries. We modify it to load the region and image captions alongside tracks. Then for each track, we either integrate those into the model forward pass (if we implemented full fusion) or compute separate similarities (if late fusion). The output will be the set of track IDs that are referred. We compare these against ground truth referred tracks to compute metrics (HOTA, DetA, AssA as in the paper).

Since we aim for **scientific rigor**, we will evaluate whether the integration indeed improves performance. We will run baseline iKUN (YOLOv8+CLIP) vs. LLMDet-integrated iKUN on Refer-KITTI. We expect to see higher **Detection Accuracy (DetA)** due to better detection recall, and hopefully higher **Association Accuracy (AssA)** and overall HOTA due to better identification of the correct object via captions. We’ll also analyze failure cases: e.g., if LLMDet generates an incorrect caption that misleads the model, or if the slower detector affects real-time performance.

## Feasibility Analysis and Experimental Plan

**Feasibility:** The proposed integration is practical with current technology. Both iKUN and LLMDet are implemented in PyTorch, making it straightforward to modify modules and pass data between them. There is no theoretical incompatibility: iKUN was explicitly designed to work with any tracker/detector output ([iKUN: Speak to Trackers without Retraining](https://arxiv.org/html/2312.16245v2#:~:text=work%2C we propose an i,Extensive)) ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=AssA%2C respectively,trained once for multiple trackers)). LLMDet extends the detector’s capabilities without requiring end-to-end retraining of the tracker, which aligns with iKUN’s design of freezing the tracker ([iKUN: Speak to Trackers without Retraining](https://arxiv.org/html/2312.16245v2#:~:text=Referring multi,noise based on the current)). The main computational cost is running LLMDet, which may be heavier than YOLOv8. However, for dataset experiments this is manageable. If needed, we can use a smaller LLMDet variant (the paper’s model zoo lists a Swin-T backbone model ([Update README.md · fushh7/LLMDet at c5523ed](https://huggingface.co/fushh7/LLMDet/commit/c5523ed2f7d4fa841760d6babb424dc8e8f8ba85#:~:text=%2B | Model | AP,sub>)) that likely runs faster). The tracking and referring modules are light in comparison.

One consideration is **inference speed**: YOLOv8 is real-time; LLMDet (with a large backbone and possibly a caption decoder) will be slower. For experiments on KITTI (small number of sequences), this is fine. If deploying in real-world, one might optimize LLMDet or only run captioning on keyframes. Another consideration is the **accuracy of LLMDet’s captions** – they won’t be perfect. However, even imperfect captions can provide useful cues (they tend to get object category and attributes right, occasionally messing up relationships). We will monitor cases where caption text might be misleading (e.g., calling a “blue car” as “gray car”) – the model might then score incorrectly. Our system can be made robust by not relying solely on the caption: we always combine it with visual matching. The weight `α` for caption similarity can be tuned low if we find captions are noisy.

**Experimental Pipeline:** We plan the following experiments on the Refer-KITTI dataset to validate the approach:

1. **Baseline Reproduction:** Run the original iKUN (with YOLOv8 detections and CLIP features) on our setup to ensure we match the reported numbers ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=9.03 46.40 ByteTrack,71)).
2. **LLMDet Integration Without Captions:** Swap YOLOv8 with LLMDet for detection+tracking, but do *not* use the caption information in KUM (just use the visual features as before). This isolates the effect of better detection. We expect improved recall (maybe higher HOTA due to more correct tracks being present).
3. **LLMDet with Caption Fusion (Full Model):** Enable the region+image caption integration in the referring stage as described. Compare metrics to (2) to quantify the benefit of caption-guided KUM. We will specifically look at cases of attributes: does the model better distinguish similar objects when captions are used?
4. **Ablations:**
   - Turn off image-level caption context to see if it contributes significantly or if region captions alone suffice.
   - Use late score fusion vs. full attention fusion to see which yields better performance or stability.
   - Vary the weight α on caption similarity if using score fusion, or try removing CLIP visual features entirely in favor of LLMDet features to see if CLIP is still needed.
   - Evaluate on **Refer-Dance** (if data available, as iKUN authors provided it) to test on a different domain (people with clothing descriptions) – this will really test open-vocabulary ability, since YOLOv8 was limited in person attribute detection, whereas LLMDet might handle “person in red dress” vs “person in green shirt” better.

We will also maintain the **plug-and-play spirit**: the integrated iKUN + LLMDet should not require joint training of the tracker. In practice, we might need to fine-tune KUM due to new input distributions, but the tracker (LLMDet + MOT) is used as is (aside from NKF which can be trained on some tracking data independently). This means if one wanted to swap in another advanced detector in future, they could do so with minimal changes, and iKUN would similarly benefit.

**Handling Open-Set and Long-Tail Queries:** Finally, we analyze how the combined system handles truly novel descriptions. Since LLMDet was trained with a vast caption corpus (GroundingCap-1M) and supervised by an LLM ([[2501.18954\] LLMDet: Learning Strong Open-Vocabulary Object Detectors under the Supervision of Large Language Models](https://arxiv.org/abs/2501.18954#:~:text=training objectives including a standard,available at this https URL)), it has seen a wide variety of descriptions. iKUN’s calibration method uses a language model to assign pseudo-frequency to query terms ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=Then%2C the pseudo frequency of,P i wij · p)) ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=similarity score sj from iKUN,Neural Kalman Filter)). In our integration, the need for this is reduced, because LLMDet essentially brings some of that LLM knowledge directly into the vision system. If a query contains an uncommon word, our system can still succeed if LLMDet detects the object and perhaps captions it with a different word. However, we will keep the calibration step as a safety net: for example, if a query describes something very fanciful that confuses the model, calibration can prevent false positives by lowering confidence for extremely rare terms. We’ll adapt the calibration to possibly incorporate information from LLMDet’s caption vocabulary – e.g., if a query term wasn’t seen in training but LLMDet’s caption for the likely object uses a known term, we treat it as not truly “rare”. This could be done by looking up the LLMDet caption’s words in the training distribution of descriptions. Implementing this is as simple as merging the query and caption text before computing pseudo-frequency, or by adding the caption words into the set of “seen words” for frequency count.

In conclusion, the integration of LLMDet into iKUN is **technically feasible and promising**. It provides a comprehensive architecture that leverages advanced open-vocabulary detection and captioning to tackle the referring multi-object tracking task. We maintain the modular design: one can plug in LLMDet in place of YOLOv8 to immediately gain open-set tracking ability, and the referring module is enhanced (but still generic) through the use of captions. The system will be implemented in PyTorch with careful attention to preserving the decoupling of detection and referring tasks. We expect this integrated approach to outperform the original on benchmarks like Refer-KITTI, especially in scenarios with uncommon object descriptions or fine-grained attributes, thereby achieving the goal of better localization and association under open-set language queries.

## **中文翻译：将LLMDet集成到iKUN框架中的技术方案**

### 动机和预期收益

**提升开放集目标检测能力：** iKUN当前使用YOLOv8作为目标检测后端 ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=the same detection results from,85)) ，只能识别固定类别的目标。我们拟用**LLMDet**替换YOLOv8，这是一种由大型语言模型监督训练的开放词汇目标检测器。LLMDet在训练时利用区域级简短描述和图像级详细描述作为监督信号 ([[2501.18954\] LLMDet: Learning Strong Open-Vocabulary Object Detectors under the Supervision of Large Language Models](https://arxiv.org/abs/2501.18954#:~:text=training objectives including a standard,available at this https URL)) ([Update README.md · fushh7/LLMDet at c5523ed](https://huggingface.co/fushh7/LLMDet/commit/c5523ed2f7d4fa841760d6babb424dc8e8f8ba85#:~:text=With this dataset%2C we finetune,modal model%2C achieving mutual benefits)) ，因此具备强大的开放词汇检测能力，能识别YOLO未定义的新类别和长尾类别以及细粒度属性。这将使iKUN能够处理查询中提到的任意目标，即使这些目标不在原有训练类别中。例如，当查询描述“建筑车辆”或特定颜色的物体时，YOLOv8可能无法检测到，而LLMDet有望识别并定位这些目标，从而提高引用目标的检出率。

**改进语言与视觉对齐：** LLMDet独特的**图文描述生成能力**为每个检测的目标提供丰富的语义信息。LLMDet可以为每个检测到的区域生成简短的文本描述（例如“白色轿车”），并为整张图像生成详细的说明性描述 ([Update README.md · fushh7/LLMDet at c5523ed](https://huggingface.co/fushh7/LLMDet/commit/c5523ed2f7d4fa841760d6babb424dc8e8f8ba85#:~:text=With this dataset%2C we finetune,modal model%2C achieving mutual benefits)) 。将这些描述融入iKUN的指引模块（KUM）将加强视觉特征与语言查询之间的对齐。在原有iKUN中，知识统一模块KUM旨在利用文本指导自适应提取视觉特征 ([iKUN: Speak to Trackers without Retraining](https://arxiv.org/html/2312.16245v2#:~:text=work%2C we propose an i,Extensive)) ([iKUN: Speak to Trackers without Retraining](https://arxiv.org/html/2312.16245v2#:~:text=,adaptive features for each description)) ；借助LLMDet的描述，模型可以在提取视觉特征时显式参考目标的属性文本。这预期会**提升指引精度**，因为系统不仅通过嵌入特征匹配，还能直接通过语言相似度确认匹配（例如，将查询中的“白色汽车”与LLMDet区域描述“白色轿车”相匹配，置信度将更高）。

**保持即插即用的模块化：** iKUN的核心特性是在跟踪器冻结的情况下插入指引模块，实现即插即用 ([iKUN: Speak to Trackers without Retraining](https://arxiv.org/html/2312.16245v2#:~:text=Referring multi,noise based on the current)) ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=AssA%2C respectively,trained once for multiple trackers))。我们集成LLMDet时将延续这一理念：仅替换检测+跟踪部分为LLMDet+MOT跟踪，而**无需修改iKUN主体**。事实上，iKUN论文实验证明，更强的检测器能提升整体性能——将YOLOv8换成Deformable DETR后，Refer-KITTI上的HOTA从约44.5%提高到48.8%，且无需重新训练iKUN ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=AssA%2C respectively,trained once for multiple trackers)) 。因此，使用性能更优的LLMDet（其开放集检测效果显著优于基线 ([[2501.18954\] LLMDet: Learning Strong Open-Vocabulary Object Detectors under the Supervision of Large Language Models](https://arxiv.org/abs/2501.18954#:~:text=training objectives including a standard,available at this https URL))）预期也能提升iKUN的跟踪和指引指标。我们通过保持接口一致来维持模块化：跟踪器输出目标轨迹（以及可选的描述）供iKUN使用，后者输出最终的指引结果。这样组合系统仍然**灵活可插拔**——如果以后需要更换检测器，只要提供相同形式的输出即可，LLMDet在此架构中扮演的是可插拔增强模块的角色。

**应对长尾描述：** iKUN提出了相似度校准来解决开放集长尾文本描述问题 ([iKUN: Speak to Trackers without Retraining](https://arxiv.org/html/2312.16245v2#:~:text=Concretely%2C a knowledge unification module,Extensive)) ，通过基于描述词频调整置信度。而集成LLMDet可以更直接地缓解此问题。LLMDet生成的**图像级描述**提供了场景的全面描述（通常使用常见词汇），区域级描述则将每个候选目标的视觉内容转化为文字。如果查询使用了罕见措辞，LLMDet的描述往往会提供一个常见的同义表达，帮助iKUN理解查询。例如，查询“深红色的汽车”用词生僻，但LLMDet可能将该目标描述为“红色汽车”，用更常见的词汇表述，从而更容易匹配。由此可见，LLMDet的语言监督使系统能将长尾描述**投影到熟悉的语义空间**处理。总的来说，将LLMDet集成进来有望**提升指引跟踪的召回率和精度**，尤其在查询包含不常见目标或罕见描述时，同时继续发挥iKUN两阶段（先跟踪再指引）的高效优势 ([iKUN: Speak to Trackers without Retraining](https://arxiv.org/html/2312.16245v2#:~:text=Figure 2%3A  The motivation,adaptive features for each description)) ([iKUN: Speak to Trackers without Retraining](https://arxiv.org/html/2312.16245v2#:~:text=decouple these two subtasks%3F” In,focus on the referring subtask))。

### 架构设计概述

**原始iKUN流程回顾：** iKUN采用先跟踪后指引的两阶段框架。一个现成的多目标跟踪器首先对视频逐帧处理，输出所有候选目标的**轨迹**集合，与语言描述无关 ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=with L words,and the final outputs T)) ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=Fref in Sec,In the textual stream%2C descrip19137))。这些轨迹（每个目标跨帧的边界框序列，带有ID）随后输入iKUN的指引模块。指引模块使用双流网络（视觉流和文本流），通过知识统一模块（KUM）为每条轨迹与输入文本描述计算匹配分数 ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=T %3D ,and the final outputs T)) ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=and textual encoders from CLIP,For similarity calibration%2C pretrained))。最终根据分数选出与描述匹配的轨迹（得分低的被过滤掉） ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=to score candidates T by,Then the similarity calibration method))。原始设计中，跟踪器基于YOLOv8的检测结果，iKUN利用CLIP的可视编码器（ResNet-50）和文本编码器提取特征 ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=and textual encoders from CLIP,For similarity calibration%2C pretrained)) ，KUM则在文本指导下融合视觉特征。

([GitHub - dyhBUPT/iKUN: [CVPR 2024\] iKUN: Speak to Trackers without Retraining](https://github.com/dyhBUPT/iKUN)) *图1：原始iKUN框架示意 ([iKUN: Speak to Trackers without Retraining](https://arxiv.org/html/2312.16245v2#:~:text=work%2C we propose an i,Extensive)) ([iKUN: Speak to Trackers without Retraining](https://arxiv.org/html/2312.16245v2#:~:text=,adaptive features for each description)) 。跟踪器（如基于YOLOv8的SORT）对视频帧序列进行多目标跟踪（左），得到所有候选目标的轨迹（中，每个目标用不同颜色框表示）。然后将冻结的跟踪器输出和语言查询一同送入iKUN模块，筛选出与查询描述匹配的目标轨迹（右，橙色框高亮的车辆）。我们计划将检测后端替换为LLMDet，并利用其描述能力增强iKUN的KUM推理，但不改变整个跟踪-指引框架的基本接口。*

**用LLMDet替换YOLOv8（检测与跟踪）：** 在流水线的第一阶段，我们用LLMDet替换YOLOv8作为检测器。即，每个视频帧不再用YOLOv8检测，而改用LLMDet进行目标检测。LLMDet的检测头是开放词汇的：训练时通过生成的文本描述来监督分类，因此推理时能检测任意类别并为每个检测生成一个描述标签或嵌入 ([Update README.md · fushh7/LLMDet at c5523ed](https://huggingface.co/fushh7/LLMDet/commit/c5523ed2f7d4fa841760d6babb424dc8e8f8ba85#:~:text=With this dataset%2C we finetune,modal model%2C achieving mutual benefits)) 。多目标跟踪则采用类似原先的策略，将LLMDet各帧的检测结果送入跟踪算法（如DeepSORT、ByteTrack或iKUN提出的NeuralSORT）进行关联，形成跨帧的轨迹。具体实现中，每帧LLMDet输出的检测（包含边框坐标、置信度，以及**区域描述文本**和/或嵌入）将用于跟踪器的数据关联步骤。我们将继续使用iKUN原始跟踪部分的实现细节，包括卡尔曼滤波和匹配策略 ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=For neural Kalman filter ,tracker is termed as NeuralSORT))。值得一提的是，这里可充分利用LLMDet的**特征和语义信息**增强关联：例如，在跟踪匹配时结合LLMDet输出的视觉特征向量，甚至文本标签，提高同一目标跨帧匹配的准确性。如果需要保持系统的简单和模块化，我们初始版本将LLMDet视作普通检测器来用，即主要依据运动和IOU进行关联（类似ByteTrack/DeepSORT），同时可以尝试使用LLMDet提供的ROI特征作为DeepSORT的外观向量来辅助判断。完成此阶段后，我们得到了一组**候选轨迹**（带有ID的目标轨迹集合），相较原先YOLOv8提供的轨迹，现包含更多种类的目标以及它们的文本描述，为下一步的语言指引奠定基础。

**将LLMDet描述融入KUM：** 集成的核心在于利用LLMDet的**区域级和图像级字幕**来增强KUM的功能和指引用推理。原始KUM中，视觉流提取全局帧特征和局部轨迹特征，并在文本指导下将二者融合，生成统一的视觉表示，再与查询文本嵌入计算相似度 ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=iKUN is designed as a,In the textual stream%2C descrip19137)) ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=and textual encoders from CLIP,For similarity calibration%2C pretrained)) 。我们对KUM作如下改进：

- **区域描述作为辅助文本输入：** 对于每个候选轨迹，LLMDet都提供了该目标的简短文本描述（如“白色汽车”或“穿红衣服的男子”）。我们将这一描述（或其文本嵌入）作为额外语义输入传递给指引模块。具体来说，可以扩展KUM为**三流交互**：即引入“轨迹视觉特征-查询文本特征-区域描述文本特征”三方的信息融合。区域描述实际上是由视觉提取得到的“机器生成文字”，如果与查询描述表达一致，将有助于提高该轨迹的匹配评分。一种设计是在KUM中采用**级联注意力机制**（cascade attention，iKUN论文中验证该策略效果最好 ([iKUN: Speak to Trackers without Retraining](https://arxiv.org/html/2312.16245v2#:~:text=Cascade attention,are first aggregated via cross)) ([iKUN: Speak to Trackers without Retraining](https://arxiv.org/html/2312.16245v2#:~:text=match at L538 Finally%2C we,the default design of KUM))），分两步融合文本信息：第一步，用查询文本对轨迹的视觉特征进行注意力变换，获得针对该查询的视觉表示（这相当于原始KUM已经做的）；第二步，再用区域描述的文本嵌入对第一步得到的视觉表示进行进一步调整或验证。如果区域描述和查询在语义上高度一致，这一步将强化相关特征（例如，如果两者都提到“红色”，则突出视觉特征中的红色属性）；若描述和查询有出入，则可能削弱该候选轨迹的总体匹配度。这样就将LLMDet的语言知识融入视觉特征提取过程中，实现了更充分的“知识统一”。在PyTorch实现中，我们可通过顺序的多头注意力层来实现：先有文本A（查询）对视觉特征做交互，接着文本B（区域描述）再与前一步输出交互。另一种简单实现是将区域描述文本直接与查询文本**拼接**后输入文本编码器，把它们当作一个复合查询来引导视觉特征提取（例如，将查询“左侧移动的汽车”与描述“白色轿车”合并成“一辆左侧移动的白色轿车”）。这样编码得到的文本特征自然同时包含了查询和目标自述的信息，只有当二者匹配时该特征才会与视觉特征高度相似，从而达到辅助判别的效果。以上两种方法我们都会实现并测试：其一通过新增注意力层在模型内部融合，其二则在输入级别融合文本，不增加模型复杂度。
- **图像级描述提供全局上下文：** LLMDet还为每帧提供了整幅图像的描述（例如：“一条街道上有几辆汽车和一名骑自行车的人，两侧有树木…”）。我们利用该全局描述来提供场景上下文的先验信息。具体而言，可将其融入KUM对视觉特征的提取指导中，方式是在文本指导中加入**图像描述的文本特征**。原iKUN视觉流已经提取了全局图像的视觉特征用于融合 ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=iKUN is designed as a,In the textual stream%2C descrip19137)) ，我们现在新增一个对应的文本版本：将图像描述通过同样的文本编码器获得`f_imgcap`嵌入向量。然后有两种使用方式：（1）将图像描述嵌入与查询嵌入进行简单融合，例如取平均或拼接后通过一层全连接，形成**考虑场景上下文的查询向量**，再用此向量作为KUM的文本指导。这样，如果查询提到的内容在图像描述中确实存在，该融合向量会更突出这些内容；反之如果图像描述没有提到查询中的对象，可能暗示目标不在场景中（当然一般假设查询描述的对象都在场）。 （2）更复杂地，用一个小Transformer让查询文本与图像描述文本做跨模态注意力，输出一个增强的查询表示。这一步可以在进入KUM前离线完成。在实现上，我们倾向于先尝试简单加权/平均，因为CLIP文本空间中的两个向量平均也是某种语义融合。如果查询用词罕见而图像描述用了常见词，两者平均可缓解偏差。例如查询“自行车手”可能是生僻词，图像描述用“骑自行车的人”，融合后模型更容易与视觉匹配。通过引入图像级描述，我们确保指引模块利用全局背景知识来理解查询，提高应对不同措辞的稳健性（如图像描述表明场景中只有两辆车一名骑车人，那么查询“骑车的人”很明确指向那名自行车手）。

**多模态指引用推理：** 结合上述改进后，指引模块将为每条轨迹输出一个相似度分数，此分数现在受到视觉特征、查询文本和LLMDet描述的共同影响。我们仍采用余弦相似度作为基本匹配度衡量，并保留iKUN的测试时校准步骤 ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=realized by temporal average pooling%2C,Net are)) ，但我们会拓展置信度的计算方式。如前所述，我们可以对视觉-文本相似度和文本-文本相似度进行加权融合。公式为：`score = w_v * S_{visual-query} + w_t * S_{caption-query}`，其中前项是原始iKUN通过KUM得到的视觉与查询的相似度，后项是LLMDet生成的描述与查询文本之间的相似度（例如用相同文本编码计算余弦相似）。通过融合，能**显式奖励**那些自身描述与查询高度相符的轨迹，提高它们的评分。在实现时，我们会通过在代码中增加相应计算，将每个轨迹的描述文本用CLIP文本编码器编码，再与查询文本向量算相似度。参数$w_v$和$w_t$可基于验证集调节，以平衡视觉证据和语言证据。在我们的设计中，视觉匹配仍是基础，文本匹配作为辅助确认。例如，在场景中有两个外观类似的人，一个穿红衣一个穿蓝衣，查询要求“穿红衣的那个人”，视觉特征上两者都属于“人”可能产生接近的视觉相似度，但加入文本匹配后，“红衣 vs 红衣”得到高分，“蓝衣 vs 红衣”分数则低，从而正确地区分目标。这种多模态融合评分方式仍输出一个最终置信度，然后与iKUN既有的校准机制结合。值得一提的是，原有的**相似度校准**基于伪词频来调整长尾描述的影响 ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=i wij · p tr,Here)) ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=similarity score sj from iKUN,Neural Kalman Filter))。在集成LLMDet后，我们预计对长尾问题的依赖降低，因为检测阶段已将视觉内容翻译成常用语言。然而，我们仍会保留校准作为保障。例如，如果某些极为罕见的描述词导致模型困惑，校准过程会降低其置信度。我们可以轻微修改校准算法以利用LLMDet的输出：例如，不仅考虑查询自身的词频，也考虑LLMDet描述中的词汇频率。如果查询用了生僻词但LLMDet描述用了常见词，我们可以认为该描述其实解除了长尾问题，从而减少对罕见查询词的惩罚。总而言之，融合LLMDet后，系统在指引用推理阶段通过视觉和语言双重线索评估候选轨迹，与原先纯视觉匹配相比，对开放集和长尾描述具有更强的鲁棒性。

综上所述，集成后的架构由三部分组成：(1) **基于LLMDet的跟踪器**：对视频帧检测并跟踪所有目标，同时生成目标描述；(2) **增强的KUM模块**：将轨迹视觉特征、查询文本特征和LLMDet描述特征共同融合，计算匹配分数；(3) **输出筛选**：结合相似度校准，挑选出最终符合查询的轨迹。通过在检测级别和特征级别充分利用LLMDet的输出，我们实现了对iKUN的全面升级。

### PyTorch集成策略

我们将在PyTorch中实现上述架构，利用现有的iKUN和LLMDet开源代码：

- **LLMDet模型集成：** 首先获取LLMDet的官方实现和预训练模型权重 ([Update README.md · fushh7/LLMDet at c5523ed](https://huggingface.co/fushh7/LLMDet/commit/c5523ed2f7d4fa841760d6babb424dc8e8f8ba85#:~:text=%2B This is the official,Laboratory%2FLLMDet)) 。LLMDet模型应该提供输入图像输出检测结果的接口（包括边框、置信度，以及可能的生成描述）。我们将编写一个PyTorch模块来封装LLMDet的前向过程，使其对每帧图像输出所需的信息。输出的数据结构可设计为包含：每个检测的`bbox`（边框）、`score`（置信度）、`region_caption_text`（区域描述字符串）和`region_feature`（区域视觉特征向量）。如果LLMDet模型直接提供了区域的特征向量，我们可以直接获取；否则可以从模型中间层提取（例如检测主干的ROI特征）。另外，还获取该帧的`image_caption_text`（整图描述）。这些输出在LLMDet的多任务模型中应是可用的——因为训练时就有检测和描述两项任务 ([Update README.md · fushh7/LLMDet at c5523ed](https://huggingface.co/fushh7/LLMDet/commit/c5523ed2f7d4fa841760d6babb424dc8e8f8ba85#:~:text=With this dataset%2C we finetune,modal model%2C achieving mutual benefits)) ，推理时很可能也能同时获得。如果需要，我们可以对每帧调用两次LLMDet（一次用于检测输出框，一次用于生成图像描述），但更优雅的是修改LLMDet的代码使其一次前向同时产生两种输出。集成此模型模块后，我们就能在视频处理循环中，每处理一帧就调用LLMDet获取检测和描述结果，从而替代原来的YOLOv8检测部分。
- **多目标跟踪实现：** 我们将PyTorch实现的DeepSORT或ByteTrack算法与LLMDet检测结果对接。iKUN的代码库已经包含NeuralSORT（即带NKF的DeepSORT）的实现 ([GitHub - dyhBUPT/iKUN: [CVPR 2024\] iKUN: Speak to Trackers without Retraining](https://github.com/dyhBUPT/iKUN#:~:text=├── CLIP ├── RN50,KITTI ├── gt_template)) ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=For neural Kalman filter ,tracker is termed as NeuralSORT)) ，我们可以在此基础上修改，使其接受LLMDet的输入。原始iKUN可能通过读入YOLOv8生成的检测文件（每帧的检测框、置信度、类别）来进行跟踪。我们将调整为直接使用LLMDet检测模块的输出（在内存中传递检测结果数据结构）。我们会忽略类别标签（开放词汇下没有固定类别ID，用不到这个信息），只基于位置和外观进行关联。对于外观特征匹配，我们有两个信息来源：**视觉特征**和**文本描述**。我们将探索两种方式：(a) 使用LLMDet的**区域视觉特征向量**作为跟踪匹配的外观embedding，类似DeepSORT中用ReID网络提取的特征向量。这利用了LLMDet模型对该目标的深度视觉表征，可区分外观不同的对象。（b）使用LLMDet的**区域描述文本**，将其编码为一个文本embedding，作为目标的语义embedding进行匹配。这种方式新颖地将跟踪的外观匹配转化为语义匹配，例如“红色汽车”的描述embedding不会与“蓝色汽车”高匹配，从语义上避免颜色混淆。我们也可以结合两者，拼接视觉特征和文本embedding形成更丰富的表示，再计算余弦相似度。我们将在PyTorch中实现可选的匹配度度量，允许配置使用视觉、文本或混合特征进行关联匹配。跟踪流程其他部分（匈牙利匹配、IOU阈值、卡尔曼预测）保持不变，与检测器无关。我们确保该跟踪模块对检测输入的依赖是通过一个统一接口，这样无论是LLMDet还是YOLO，都可以提供检测结果给跟踪器，实现真正的模块替换。
- **扩展KUM的代码实现：** iKUN的模型定义（`model.py`等）将被修改以实现前述融合策略。在原代码中，模型会加载预训练的CLIP编码器，并在`forward`中对轨迹图像和文本分别编码，然后通过某种融合机制（cascade attention等）计算相似度。我们的修改重点如下：
  1. **数据准备：** 对于每个轨迹，在进入模型前，我们从跟踪器输出中已经获得了它的区域描述文本和（可能的）全局图像描述文本。我们用与原来相同的CLIP文本编码器（冻结）分别对查询文本E、图像描述文本Tc和区域描述文本Tg编码，得到对应的嵌入向量：`f_query`、`f_imgcap`、`f_regcap`（维度Ct=1024 ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=and textual encoders from CLIP,For similarity calibration%2C pretrained))）。这些操作可以利用原有代码中CLIP编码文本的函数，只是我们现在对三段文本各调用一次编码器即可。由于CLIP文本编码器参数量不大且我们不训练它，计算开销也不高。
  2. **视觉特征提取：** 原始iKUN对每个轨迹会提取局部目标帧片段的视觉特征和对应帧的全局视觉特征。我们仍然沿用这一思路，但可考虑融合LLMDet的视觉输出。例如，我们可继续使用CLIP ResNet-50来编码轨迹对象的图像（取每个轨迹最近的一帧或多个帧裁剪区域，通过前向网络得到一个2048维特征）以及编码对应全帧图像得到全局特征图。除此之外，如果LLMDet本身有一个ROI特征（比如其检测主干网络对该区域的特征），我们可以尝试直接使用或融合该特征与CLIP特征。为了初始稳定，我们可能先保留CLIP特征流不变，后续再尝试用LLMDet特征替换或融合（这需要确保维度匹配，例如通过一个线性层将LLMDet特征变换到CLIP特征空间）。现在我们得到`f_local`（轨迹局部视觉特征）和`f_global`（帧全局视觉特征）。接下来，实现我们设计的两种方案：
     - **方案1：模型内部特征融合：** 修改KUM模块内部，使其利用我们准备的文本嵌入（查询、区域描述、图像描述）。如果原KUM采用cascade attention，我们可以增加attention层次。如前所述，先用图像描述文本向量`f_imgcap`对视觉特征进行注意力融合，再用查询向量`f_query`对融合结果进一步交互。这可在PyTorch中通过`nn.MultiheadAttention`模块的灵活应用实现：第一次attention以`f_imgcap`为查询，`f_global+f_local`为键值，更新视觉特征；第二次attention以`f_query`为查询，再次对更新后的视觉特征做键值。这两个attention层的参数需要训练。此外，我们也可以尝试并联融合：比如同时用`f_query`和`f_regcap`分别与视觉特征做交互，然后将结果融合。这种需要在代码中实现一个多分支网络结构，可能较复杂。为了降低实现难度，我们可以折中：先将`f_query`和`f_regcap`融合成一个文本向量（例如求平均或者拼接后线性变换），得到一个综合的文本指导向量`f_text_guidance`，然后用单一的cross-attention让`f_text_guidance`与视觉特征交互。这个`f_text_guidance`可以看作将“查询”和“目标自述”合并后的文本语义。以上方法均可以在PyTorch中通过矩阵运算实现，我们会测试哪种效果更好。
     - **方案2：后期分数融合：** 这种实现更为简洁，对模型结构改动小。我们不改动原KUM的attention机制，仅在得到输出分数时融合文本相似度。如前述公式，将视觉流输出的相似度分数与文本相似度合成新的分数。这在代码中可在推理阶段实现：计算出原相似度张量`S_v`（shape为[batch, num_tracks]），再计算每个(track, query)的文本相似度`S_t`（同维度），然后按权重系数加和，得到新的分数用于决策。若需要在训练中让模型学习权重或其他函数f(x)，我们可以将这个融合操作写成一个小的子模块（例如一个线性层接受文本相似度值输出一个偏移），并将其参数包括在训练优化中。但由于iKUN原本是靠调参获得校准函数f(x) ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=similarity score sj from iKUN,Neural Kalman Filter)) ，我们也可以选择不训练而在验证时调节这个α权重。因此，方案2更像一个推理级的调整，不干扰模型主干。
  3. **模型训练/微调：** 我们计划在Refer-KITTI上按照iKUN原方案训练指引模块（KUM和评分头），同时对我们新增/修改的部分进行训练调整。原iKUN训练使用了**真实轨迹的监督**（ground truth tracklets），通过给正确轨迹赋予正样本、错误轨迹为负样本来训练模型输出相应高低分 ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=realized by temporal average pooling%2C,Net are)) 。我们将延续这种训练方式。区别在于我们现在有额外的文本输入（LLMDet描述），但在训练阶段这些描述也可以由真实数据生成：Refer-KITTI数据集中每条轨迹其实对应一个目标类别（如Car或Pedestrian）和属性（颜色、位置描述等），我们可以利用这些信息构造伪描述（或者直接运行LLMDet模型离线为训练集帧生成描述作为训练输入）。例如，对每个ground truth轨迹，我们预先使用LLMDet对所在帧生成区域描述文本，将其作为训练时的`Tg`。这样，模型在训练时就接收到类似实际推理时的输入格式：轨迹的图像特征、轨迹的描述文本、图像的描述文本和查询文本。一旦准备好这些训练对，我们冻结CLIP的编码器参数，只训练KUM融合层和最后的得分层（和可能的α等）。损失仍采用二进制分类的Focal Loss，如原文所述 ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=realized by temporal average pooling%2C,Net are))。这样训练100个epoch（原文设置）后，我们期望模型学会有效利用LLMDet描述来提高正轨迹评分、压低负轨迹评分。
- **代码推理流程：** 在PyTorch代码实现完成并训练好模型后，推理阶段我们会按如下顺序：
  1. **运行检测+跟踪：** 对于测试视频序列，用LLMDet检测每帧并运行跟踪算法，得到每个序列的候选轨迹集合。我们可以将这些结果保存成文件（例如每条轨迹的边界框列表，以及我们为指引用准备的描述文本）。这一步也可以与下一步在线结合，但为了方便分析，我们可以先离线生成所有轨迹和描述。在使用NeuralSORT时，我们会应用NKF来改进跟踪效果，这需要在跟踪时根据上一帧速度动态调整卡尔曼滤波参数 ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=and Qk,time step k − 1)) ，iKUN提供的NKF模型我们已经有了，可以直接用（在YOLO基础上他们只训练了10轮，我们也可在LLMDet检测下微调NKF的Q-Net和R-Net少量epoch，以适应不同检测误差分布）。一旦轨迹生成完毕，我们确保以与原始iKUN相同的格式提供给指引模块（包括每条轨迹的帧序列、ID等），并额外提供LLMDet生成的描述。
  2. **指引模块推理：** 对于每个查询文本，我们载入对应视频的候选轨迹列表和预先生成的描述数据。模型将对每条轨迹计算匹配分数并排序。我们在代码中实现了一种批量计算的方法，利用矩阵运算以加速——例如，将所有轨迹的局部图像拼成一个batch通过CLIP计算特征，然后所有文本一次性编码。然后在KUM模块中通过广播机制计算每个轨迹的score。拿到分数后，我们按照iKUN的流程，应用我们学到或设定的校准函数f(x) ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=similarity score sj from iKUN,Neural Kalman Filter))调整分数，然后筛选出高于阈值的轨迹作为结果输出。如果数据集中每个查询只对应一个目标，我们就选择最高分的轨迹输出（通常Refer-KITTI是可能有多个满足描述的目标，比如“所有行驶中的车”，那就输出所有车轨迹的分数超过一定值者）。

整个推理流程与原iKUN基本一致，只是在前端换了检测器，在后端融合了描述文本。但对用户而言，这仍旧是一个可插拔模块架构：他们可以像使用原iKUN一样提供视频和查询，系统自动利用LLMDet完成检测、跟踪和指引，无需手工干预中间步骤。

### 可行性分析与实验计划

**技术可行性：** 所提方案在当前技术条件下是可行的。iKUN和LLMDet都有开源PyTorch实现，可通过修改模型和编写胶合代码将二者串联。框架上没有不可克服的障碍：iKUN支持更换任意跟踪器 ([iKUN: Speak to Trackers without Retraining](https://arxiv.org/html/2312.16245v2#:~:text=work%2C we propose an i,Extensive))，LLMDet提供了增强的检测能力且输出包括语言信息，这恰好符合iKUN所需的输入形式（目标轨迹加文本提示）。我们主要需要注意计算效率和资源占用。YOLOv8非常高效，而LLMDet模型（例如使用Swin Transformer和大语言模型头）可能较重。幸而我们的实验场景（Refer-KITTI）数据规模不大，完全可以离线运行较慢的模型。即使需要加速，我们也可考虑使用LLMDet的轻量版本（据论文提供的模型Zoo，有Swin-T小模型 ([Update README.md · fushh7/LLMDet at c5523ed](https://huggingface.co/fushh7/LLMDet/commit/c5523ed2f7d4fa841760d6babb424dc8e8f8ba85#:~:text=%2B | Model | AP,sub>))）或减少描述生成的频率（比如每隔帧生成一次描述）。**存储**方面，保存所有轨迹的描述文本也是很轻量的（文本信息相比视频像素量不值一提）。另一个潜在问题是**LLMDet描述的准确性**：如果描述有误差，可能误导指引决策。为降低风险，我们在融合时采取“视觉+文本”并重的方法，而不完全依赖描述文本。这样，即便某轨迹的描述稍有偏差，只要视觉特征匹配度高，仍会被选出；反之，如果描述匹配但视觉明显不符，也不会通过。在实际中我们会观察描述质量，比如LLMDet将“蓝色”错描述为“灰色”的情况，可能导致针对颜色的查询出错。对此，我们可以降低文本匹配分数的权重，或者改进LLMDet模型（若有可能微调其描述生成）。总的来说，通过合理调参，系统对描述误差有一定鲁棒性。

**实验计划：** 我们将在Refer-KITTI数据集上进行以下实验来验证方案效果：

1. **基线复现：** 首先运行原始iKUN（YOLOv8 + CLIP方案）以复现论文中的指标，确保我们的环境设置正确 ([iKUN: Speak to Trackers without Retraining](https://openaccess.thecvf.com/content/CVPR2024/papers/Du_iKUN_Speak_to_Trackers_without_Retraining_CVPR_2024_paper.pdf#:~:text=9.03 46.40 ByteTrack,71)) 。这包括使用作者提供的YOLOv8检测结果或自行生成、然后跑iKUN指引，计算HOTA、DetA、AssA等指标。
2. **仅换检测器（不使用描述）：** 在不改动指引模块的情况下，用LLMDet+NeuralSORT替换原YOLOv8+DeepSORT，观察性能变化。这可以量化“更强检测和更多目标”对结果的贡献。我们预计DetA（检测准确度）和HOTA会有所提升，因为LLMDet检测到了更多Ground Truth目标轨迹。
3. **启用描述融合的完整模型：** 按我们设计的方式融合LLMDet的区域和图像描述，然后评估指标相较步骤2的提升。我们将特别关注AssA（关联准确度）和描述相关的细粒度指标：如针对颜色、动作描述的查询，模型成功率是否提高。通过案例分析，验证描述文本帮助模型更好地区分相似目标或更准确地选择符合描述的目标。
4. **消融实验：** 为了深入理解各模块作用，我们将进行一些消融测试：
   - **去除图像描述：** 禁用全局图像caption的融合，仅用区域描述，看性能差异。这验证场景上下文是否提供了额外帮助。
   - **去除区域描述：** 反过来，只用图像caption和视觉，不用区域caption，观察效果，从而了解区域文本的重要性（预计区域描述更关键，因为它针对具体目标）。
   - **评分级融合 vs 特征级融合：** 如果我们实现了两种融合方案，我们将比较它们的表现。可能特征级融合效果更好但训练更复杂，而简单后期融合可能已经能抓住大部分好处。我们也关注它们对保持模块化的影响：评分级融合几乎不改模型结构，更利于今后插拔。
   - **不同权重设定：** 调整文本相似度在总分中的权重α，或者如果用了校准函数f(x)，观察不同参数对结果的影响。理想情况下，我们希望存在一组稳健的参数，使文本辅助带来明显收益且不会引入许多误检。
   - **不同检测器比较：** 为进一步验证iKUN框架的灵活性和我们方案的优越性，我们可以将另一开放集检测方法（例如YOLO-World或其他OVD模型 ([YOLO-World Model - Ultralytics YOLO Docs](https://docs.ultralytics.com/models/yolo-world/#:~:text=YOLO,Vocabulary Detection tasks))）替换进来，与LLMDet效果对比。如果LLMDet表现更好，将证明其强大的检测和描述能力确实对RMOT任务更有利。

在评估过程中，我们特别关注**开放集描述**和**长尾词汇**的测试。例如，Refer-KITTI的描述通常涉及车辆和行人，但我们会挑出那些包含颜色、位置、动作等长尾属性的描述，看我们的系统能否正确响应。这些也是原iKUN标明的难点 ([iKUN: Speak to Trackers without Retraining](https://arxiv.org/html/2312.16245v2#:~:text=localization accuracy%2C we present a,Extensive))。有了LLMDet描述，系统在这些情况下的表现应当改进。我们也计划测试iKUN作者新提供的Refer-Dance数据集（如果有权限），其中描述涉及服装颜色和舞蹈动作。这将是开放集多目标跟踪更具挑战性的场景。我们预计LLMDet在该场景依然有优势，因为其训练语料包含大量人和服装的描述，可以检测出“穿红T恤的人”、“穿绿裤子的人”等，而原YOLOv8检测器无法直接输出这类属性。

**即插即用模块化的保持：** 在整个实现和实验中，我们都会确保架构的模块化。例如，在我们的代码中，会把LLMDet+跟踪器封装成一个类，输出结果格式与之前YOLO+跟踪器相同。这意味着，虽然我们专注于将LLMDet融合优化，但如果有用户想换回YOLO或其他模型，只需提供对应检测结果文件或通过统一接口调用，不影响iKUN指引部分。类似地，指引模块虽然融合了描述文本，但如果在某些情况下没有描述输入（例如使用传统检测器无法提供区域caption），我们的实现也应能优雅降级，至少还能基于视觉进行匹配。我们可以在代码中做判断：如果track没有文本描述，跳过相关融合，直接用视觉相似度即可。这样，系统仍然兼容不同的检测器插件。这一点保证了我们的解决方案符合iKUN“无需重新训练即可对接不同跟踪器”的初衷 ([iKUN: Speak to Trackers without Retraining](https://arxiv.org/html/2312.16245v2#:~:text=Referring multi,noise based on the current))。

综上，我们的方案在PyTorch中具有很高的可实现性，并计划通过严谨的实验验证其有效性。通过将LLMDet融入iKUN，我们预期能大幅提升Refer-KITTI等数据集上的指引多目标跟踪性能，在保持原有框架灵活性的同时，让系统能够“看懂”更丰富的语言描述并找到对应目标。这将把RMOT任务推进一步，朝着真正开放集的、多样化描述的实时跟踪迈进。